## 2 millions news corpus for Vietnamese NLP task

All data was removed dupplicate, invisible space, ....

MongoDB (all information: author, images, cover, ....): ~6GB uncompressed\
[Download](https://drive.google.com/file/d/1gTFdON-3DFL1HJ-01VXfmmPUzmdNPLzX/view?usp=sharing)

MongoDB demo\
![dantri demo.png](https://github.com/Avi197/Vietnamese-news-corpus/blob/main/mongo%20demo.png)
![detail demo.png](https://github.com/Avi197/Vietnamese-news-corpus/blob/main/detail%20demo.png)


title and description only (classification): ~500MB uncompress\
[Download](https://drive.google.com/file/d/1tavVhYYqMwdbH3fnqNcNCXUMO0IJO2-1/view?usp=sharing)

Raw text\
![non tokenized demo.png](https://github.com/Avi197/Vietnamese-news-corpus/blob/main/non%20tokenized%20demo.png)

Tokenized text\
![tokenized demo.png](https://github.com/Avi197/Vietnamese-news-corpus/blob/main/tokenized%20demo.png)

Title, description, content tokenized (raw text): ~5GB uncompressed, ~1GB compressed\
[Download](https://drive.google.com/file/d/1pXeX8YFOE1BRpKusAYFdBmjO6X9IH-g2/view?usp=sharing)


There is a bigger news corpus by binvq with different news source, contain around 14 millions news (raw, not preprocessed), use that one if you need a lot of data\
[Binhvq news corpus](https://github.com/binhvq/news-corpus)
